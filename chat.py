import os

import streamlit as st
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langsmith import Client
from langchain_core.runnables import RunnablePassthrough
from pinecone import Pinecone
from operator import itemgetter

st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ’¬")
st.title("ğŸ’¬ ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")

load_dotenv()

# Pinecone
pinecone_api_key = os.environ.get("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)

# LangSmith
langsmith_api_key = os.environ.get("LANGSMITH_API_KEY")
langsmith_client = Client(api_key=langsmith_api_key)

# Embeddings & Vector Store
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
index_name = "tax-index-large"
database = PineconeVectorStore(embedding=embeddings, index=pc.Index(index_name))

# LLM
llm = ChatOpenAI(model="gpt-4o-mini")

def get_ai_message(user_input):
    dictionary = [
        "ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„(ì˜ˆ: ì‚¬ëŒ, ì¸ê°„, ê°œì¸, ì‹œë¯¼, ì£¼ë¯¼, ë‚©ì„¸ì) â†’ ê±°ì£¼ì",
    ]

    dictionary_prompt = ChatPromptTemplate.from_template(
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ì•¼ë˜ ì œê³µëœ ì‚¬ì „ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì¼ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´, ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        
        # ì£¼ì˜ì‚¬í•­
        - ì„¤ëª…, ì ‘ë‘ì–´, ì ‘ë¯¸ì–´, ë”°ì˜´í‘œë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ê²ƒ
        - "ë³€ê²½ëœ ì§ˆë¬¸", "ìˆ˜ì •ëœ ì§ˆë¬¸" ë“±ê³¼ ê°™ì€ ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì§€ ë§ê²ƒ
        
        ì‚¬ì „:
        {dictionary}
        
        ì§ˆë¬¸:
        {question}
        """
    ).partial(dictionary="\n".join(dictionary))
    dictionary_chain = (
        dictionary_prompt 
        | llm 
        | StrOutputParser()
    )

    qa_prompt = langsmith_client.pull_prompt("teddynote/rag-prompt-korean", include_model=True)
    qa_chain = (
        dictionary_chain
        | {
            "context": database.as_retriever(search_kwargs={"k": 4}),
            "question": RunnablePassthrough(),
        }
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    return qa_chain.invoke(user_input)


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

if user_input := st.chat_input(placeholder="ì†Œë“ì„¸ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"):
    with st.chat_message("user"):
        st.write(user_input)

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        ai_message = get_ai_message(user_input)

        with st.chat_message("ai"):
            st.write(ai_message)

        st.session_state.messages.append({"role": "ai", "content": ai_message})
